pytorch
#pytorch中需要构建计算图
Tensor(张量)，是Pytorch中的基本操作对象，可以看做是包含单一数据类型元素的多维矩阵。
从使用角度来看，Tensor与Numpy的ndarray非常类似，相互之间也可以自由转换，只不过Tensor还支持GPU加速。
pytorch中的数据格式是一个Tensor的格式
tensor=torch.empty(5,3)
#创建一个5行3列的随机值矩阵
tensor=torch.rand(5,3)
#创建一个和tensor1形状一样的随机值张量
tensor2=torch.randn_like(tensor1,dtype=torch.float)
#生成服从正态分布的随机数张量
x=torch.normal(mean, std，size=())
该函数返回从单独的正态分布中提取的随机数的张量，该正态分布的均值是mean，标准差是std。




#常见tensor的形式
1.scalar-标量(0-d)
2.vector-向量(1-d)
3.matrix-矩阵(2-d)(行-simple、列-feature)
4.3-d(图片hwc)
5.4-d(批量图片bhwc)
6.5-d(一个视频批量bthwc)
4.n-dimensional tensor-高维特征






访问元素








tensor中包括两部分：data和grad
data：本身的值
grad：损失函数对权重的导数
#对需要计算梯度的tensor进行声明
tensor.requires_grad=True
默认的tensor创建后是不需要计算梯度的，构建计算图后这个tensor不会计算梯度。
非tensor和tensor1计算后得到后的结果是一个tensor2，如果tensor1中包含梯度，那么tensor2也有梯度



#访问张量的形状
tensor.shape()
#改变张量的形状而不改变元素数量和元素值
x=x.reshape(3,4)#将元素改变为3行4列的矩阵
#张量的维度
tensor.dim()
#张量中元素的个数
tensor.size()




#特殊矩阵的创建
#初始构建一个5行3列的全零的矩阵
x=torch.zeros((5,3))
#创建一个各元素为1的3行4列矩阵张量
x=torch.ones((3,4))
#使用包括数值的python列表(或嵌套列表)可以为张量中的每个元素赋予确定值
x=torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])#二维数组





#将张量转换为python数字(转换后的张量没有梯度，不能进行反向传播)
tensor.item()

#张量的运算(对应元素的计算)
#张量的加法
(1)tensor1+tensor2
(2)torch.add(tensor1,tensor2)
+-*/**(加减乘除幂)
如果有浮点数参与计算，数据会变为浮点数
#张量的点积
(1)torch.dot(x,y)
(2)torch.sum(x*y)
#矩阵张量的乘法
y=torch.manual(x,w)+b
#张量的L1范数是向量各元素的绝对值之和
torch.abs(x).sum()
#张量的L2范数是向量平方和的平方根(也就是向量长度)
torch.norm(x)
#矩阵张量的F范数是矩阵元素的平方和的平方根
torch.norm(x)
#张量的转置
x.T





#张量的复制
tentor.copy()
tentor.clone()







#张量的连结
torch.cat((x,y),dim=0)#将tensorx和tensory按行合并(行变多)
torch.cat((x,y),dim=1)#将tensorx和tensory按列合并(列变多)


#对张量中的所有元素进行求和会产生一个只有一个元素的张量
x.sum()



#广播机制
在维度一致的情况下，即使形状不同，我们仍然可以通过调用广播机制(数值复制)来执行按元素操作







#一些操作肯能导致为新结果分配内存
如果在后续计算中没有重复使用x，我们可以使用x[:]=x+y或x+=y来减少操作的内存开销

#与numpy的协作
(1)tensor转换为ndarry
a=tensor.numpy()
(2)ndarry转换为tensor
tensor=torch.from_numpy()


#将大小为1的张量转换为python张量
a=torch.tensor([3,.5])
a.item()-----3.5(numpy中的浮点数)
float(a)-----3.5(python中的浮点数)
int(a)-------3





，;
#加载和保存张量
torch.save(x,'要保存的文件名')
x：可以是一个张量,可以是一个张量列表，可以是从字符串映射到张量的字典
torch.load('要加载的文件名')









#数据预处理
创建一个人工










#view操作可以改变矩阵的维度
x=torch.rand(4,4)
y=x.view(16)
z=x.view(-1,8)
#x的size是[4,4],y的size是[16],z的size是[2,8]
这里的-1是能能够通过元素个数和其他维度的大小推断出来的





#reshape和view并没有太多本质上的区别
b是由areshape得到的，这样a和b都会改变





网络层
#线性层
nn.Linear(输入层，输出层)
#展平层
nn.Flatten()：将高维数组展平为一个一维行向量
#卷积层
nn.Conv2d(c01，c02，kernel_size=()，padding=()，stride=)
-c01:输入通道数
-c02:输出通道数
-kernel_size:卷积核尺寸(hw)
-padding:填充尺寸(hw)
-stride:步长
#最大池化层
nn.MaxPool2d(n，padding=,stride)
-n:n是尺寸为n*n的窗口
-padding:填充
-stride:步长




#独热编码
features=pandas.get_dummies(features)
pandas会对数据中的字符串进行独热编码，并拼接到数据中







tensor.randn(x,y,requires_grad=True)




#自动求导的实现
在计算x的梯度之前，我们需要一个地方来存储梯度
x.requires_grad=True
#通过调用反向传播函数可以自动计算loss关于x的每个分量的梯度
loss.backward()
#在默认情况下，pytorch会累计梯度，我们需要清除之前的值
x.grad.zero()




#自动求导机制：
一.定义张量是否需要求梯度
方法一：tensor=torch.randon(x,y,requires_grad=True)
方法二：tensor=torch.randon(x,y)
            x.requires_grad=True
#即使一些张量没有定义需要梯度，如果后续需要，会自动求梯度
每次调用一次loss函数，系统就会动态构建一次计算图
二.查看一个张量的梯度
tensor.grad()
三.反向传播
loss.backward()会对所有需要求梯度的tensor自动求梯度，并保存到变量中，之后释放计算图
tensor.backward()#如果梯度不清理，多次执行程序


网络的结构
组成构件：块和层

层中参数的指定，可以用nn.Parameter()指定


带有梯度的tensor在计算（如定义前馈、loss）时：会构建计算图
backward：对计算图反向传播，对需要梯度的张量求梯度，之后释放掉计算图

计算图如果过大会占用巨大的内存
防止构建计算图的方式，使用纯数据不用张量
(1)tensor.data
(2)tensor.item()








#torch.autograd和Variable
torch.autograd包的主要功能就是完成神经网络后向传播中的链式求导，手动去写这些求导程序会导致重复造轮子的现象。

自动梯度的功能过程大致为：先通过输入的Tensor数据类型的变量在神经网络的前向传播过程中生成一张计算图，然后根据这个计算图和输出结果精确计算出每一个参数需要更新的梯度，并通过完成后向传播完成对参数的梯度更新。

完成自动梯度需要用到的torch.autograd包中的Variable类对我们定义的Tensor数据类型变量进行封装，在封装后，计算图中的各个节点就是一个Variable对象，这样才能应用自动梯度的功能。

反向传播计算
tensor.bacjware(retain_graph=True)#





#访问cpu
torch.device('cpu')
#访问第一块GPU
torch.cuda.device('cuda:1')

#查询可用GPU的数量
torch.cuda.device_count()


#指定使用GPU进行计算
device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')#指定使用GPU0运算
model.to(device)
inputs=inputs.to(device)
labels=outputs.to(device)




#访问线性模型参数
从容器中收集
1.访问权重和偏差：网络实例对象[].state_dict()
2.访问偏差：网络实例对象[].bias(如果只看值，可以加.date，如果只看梯度，可以加.grad)
或：网络实例对象.state_dict()['指定线性层(默认从0开始).bias'].data
#一次性访问网络的所有参数
网络实例对象[]named_parameters()
返回值:name和param.shape
从嵌套块中收集
print(网络实例对象名)







内置初始化(初始化参数)
多种方法，常用正态分布




#参数共享(参数绑定)
绑定后的神经网络层不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 
当参数绑定时，由于模型参数包含梯度，在反向传播期间两个绑定层的梯度会加在一起。











#加载和保存模型参数
#保存模型参数
torch.save(网络对象名.state_dict(), '参数文件名')
#加载模型参数
clone=网络名
clone.load_state_dict(torch.load('参数文件名'))
clone.eval()#eval()是进入评估模式，不计算梯度
#使用加载的模型参数
Y_clone=clone(x)






























